<!DOCTYPE html>
<html><head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="open-dogc.GitHub.io : ">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/styles.css">

    <title>Open DOGC</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/open-dogc"></a>

          <h1 id="project_title">Open DOGC</h1>
          
          <ul class="menu-nav">
            <li><a href="index.html">Home</a></li>
            <li><a href="data.html">Data</a></li>
            <li><a href="classification.html">Classification</a></li>
            <li><a href="visualization.html">Visualization</a></li>
          </ul>
        </header>
    </div>

   <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
		<div id="main_content" class="inner">
			

			<span class="anchor" id="data"></span>
			<div class="section">
				<h2>Data</h2>
        
        <p>To obtain official documents that are available in the website we need to scrap them. To do that we used the advanced search, that allowed us to be more specific about the kind of documents we wanted to analyze.</p>
        
        <p>Documents are written in Catalan, although they can also be downloaded in Spanish. However, the Spanish download returns part of the text in Catalan. Catalan is a difficult language to apply cleaning packages of text in python, and a text with two languages also implies other challenges. To solve that, we translated all documents to English in order to apply the cleaning techniques available in python. Translation was performed using textblob.</p>


        <p>We did two kinds of search. One of all kind of documents and another one by specific names of politicians.</p>

        <p>Documents downloaded were stored in a MongoDB dataset. After that, the text inside the document was cleaned in order to be analyzed. Data cleaning was performed in several steps:</p>

				<ol>
				  <li>Tokenization</li>
				  <li>Removal of punctuation</li>
				  <li>Removal of unnecessary words</li>
				  <li>Stemming of words</li>
				</ol>


				<p>The python packages to apply this points were numpy, pandas, nltk, string and textblob.</p>
			</div>
	
		</div>
	</div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

  

</body></html>
